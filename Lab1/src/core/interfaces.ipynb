{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f439fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3c03d3",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970070bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \n",
    "    def tokenize(self, text:str)-> List:\n",
    "        '''Tách một chuỗi đầu vào thành 1 list các token'''\n",
    "        \n",
    "        tokens = re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE)\n",
    "        return tokens\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8ac6a7",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb7f0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \n",
    "    def tokenize(self, text:str)-> List:\n",
    "        '''Tách một chuỗi đầu vào thành 1 list các token'''\n",
    "        \n",
    "        tokens = re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE)\n",
    "        return tokens\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6f43736",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer:\n",
    "    def __init__(self):\n",
    "        self.vocab = {}\n",
    "\n",
    "    def Fit(self, corpus: List[str]) -> None:\n",
    "        '''Học từ vựng từ tài liệu đầu vào'''\n",
    "        idx = 0\n",
    "        for doc in corpus:\n",
    "            for word in doc.split():\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab[word] = idx\n",
    "                    idx += 1\n",
    "\n",
    "    def transform(self, corpus: List[str]) -> List[List[int]]:\n",
    "        '''Biến đổi tài liệu đầu vào thành danh sách các vector'''\n",
    "        vectors = []\n",
    "        for doc in corpus:\n",
    "            vector = [0] * len(self.vocab)\n",
    "            for word in doc.split():\n",
    "                if word in self.vocab:\n",
    "                    vector[self.vocab[word]] += 1\n",
    "            vectors.append(vector)\n",
    "        return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "957c474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text0 = \"Hello, world!\"\n",
    "text1 = \"Hello, world! This is a test.\"\n",
    "text2 = \"NLP is fascinating... isn't it?\"\n",
    "text3 = \"Let's see how it handles 123 numbers and punctuation!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d273e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world! -> ['Hello', ',', 'world', '!']\n",
      "Hello, world! This is a test. -> ['Hello', ',', 'world', '!', 'This', 'is', 'a', 'test', '.']\n",
      "NLP is fascinating... isn't it? -> ['NLP', 'is', 'fascinating', '.', '.', '.', 'isn', \"'\", 't', 'it', '?']\n",
      "Let's see how it handles 123 numbers and punctuation! -> ['Let', \"'\", 's', 'see', 'how', 'it', 'handles', '123', 'numbers', 'and', 'punctuation', '!']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "print(f\"{text0} ->\" , tokenizer.tokenize(text0))\n",
    "print(f\"{text1} ->\" , tokenizer.tokenize(text1))\n",
    "print(f\"{text2} ->\" , tokenizer.tokenize(text2))\n",
    "print(f\"{text3} ->\" , tokenizer.tokenize(text3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
